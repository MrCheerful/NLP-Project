---
title: "Predictive Text App - Milestone Report"
author: "MrCheerful"
date: "May 3, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2); library(parallel); library(dplyr); library(RWeka); library(magrittr); library(tm); library(stringi); library(slam)
```

#### John Hopkins Data Science Certificate Capstone Project


### Report Objectives

This report summarizes progress on the predictive text algorithm development. The focus for the first two weeks of activity has been establishing the development environment, reviewing the available data, and becoming familiar with the computational routines available for working with the textual datasets.


### Data Retrieval and Cleaning Operations

For the developement of the predictive text algorithm the JHU web scraping team has provided several data collections in four languages and from three sources with varying levels of formality.  For this project, development will be limited in scope to using the English datasets.   

The raw data may be retrieved from the Coursera site:  [Capstone Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)

#### Loading Dataset

For brevity, the code chunk for downloading and extracting the data is not shown.  From the extracted data sets, the relevant datasets are readily retrieved using functionality from the tm library.
 
```{r}
# Set up dataset retrieval options
dd <- "./Coursera-SwiftKey/final/en_US"
dd.texts <- DirSource(directory=dd, encoding='UTF-8', mode="text" )

# Read data files into a volatile (in memory) corpus
corp <- VCorpus(dd.texts)
```


#### Initial Dataset information

The raw data is summarized with the following metrics.

```{r echo=FALSE}
library(stringi)
cor.info <- function(c) {
      # set up initial empty vectors  
      x1 <- character() ; x2 <- numeric() ; x3 <- numeric() ; temp <- numeric()
      x4 <- numeric() ; x5 <- numeric()
      # loop through datasets in corpus collecting metrics
      for (i in 1:length(c)){
            x1[i] <- meta(c[[i]])$id    # descriptive id of dataset in corpus
            x2[i] <- file.size(paste(dd,x1[i],sep="/"))  # size of dataset's file on disk
            x3[i] <- object.size(c[[i]])    # size of dataset in memory
            temp <- stri_count_words(c[[i]])   # make vector of word count by line
            x4[i] <- length(temp)   # number of lines
            x5[i] <- sum(temp)      # total number of words
      }
      x6 <- x5/x4     # calculate average words per line.
      # return metrics as dataframe
      data.frame('Files'=x1, 'FileSize'=x2, 'MemSize'=x3, 'Lines'=x4, 'TotalWords'=x5, 'AvgWords'=x6)
}
cor.info(corp)
```


#### Text Example

The following example is a small extract from one of the datasets prior to cleaning activities.

```{r}
# set up function for viewing data.  Acknowledgement to Graham@togaware for function
cor.view <- function(d,n){ d %>% extract2(n) %>% as.character() } # %>% writeLines() }
cor.view(corp,1)[5]
```


### Bad Words Dataset

As the intended audience of the application is the general public, the predictive algorithm shall not suggest socially inappropriate terms.  As the end users may span the full range of cultural sensitivities, it is best to err on the safe side and exclude any potentially offensive terms.  There are no restricitions on an individual user's expressiveness; if a user wishes to use an obscene or offensive word, they are able to type it in as usual.    

From a quick internet search, Luis von Ahn has a fairly comprehensive list of potentially offensive words at: [List of bad words](https://www.cs.cmu.edu/~biglou/resources/bad-words.txt)
This will be used as the starting point and may be readily expanded as further experience is gained.
```{r, echo=FALSE}
corp <- VCorpus(VectorSource("Hello, my name is long name for text analysis"))
```

The tm library command for filtering the profanity has been tested and is included in the code chunk.  

```{r }
if(!file.exists("bad-words.txt")){
      download.file("https://www.cs.cmu.edu/~biglou/resources/bad-words.txt", "bad-words.txt")
}
BadWords <- readLines(con="bad-words.txt", warn=FALSE, encoding='UTF-8' )
# strip empty lines from BadWords
BadWords <- BadWords[BadWords!=""]

# remove bad words
corp <- tm_map(corp, removeWords, BadWords)
```


### Data Splitting

The dataset has been seperated as follows:

* Training data - 70%
* Test set 1 - 15%
* Test set 2 - 15%

In addition, small sample sets from the training data have been taken for use during code development.


### Possible manipulations

The exploratory analysis included experimenting with the possible manipulations of the data using available routines within the tm and weka libraries.  Available manipulations include:

* Remove whitespace
* Convert to lower case
* Remove non-english characters
* Remove stopwords or bad-words
* Remove numbers and punctuation
* Find and replace particular strings or characters


### Descriptive Statistics on Datasets

A variety of visuals are presented to illustrate aspects of the data set.  The data is first tabulated as unigrams into a term document matrix, from which the visuals are derived.

```{r, echo=FALSE}
load("./data-4-cleaned-1/tdm1-train25.Rdata")


```

#### Histogram of Common Words

```{r}
# convert TDM to dataframe
corp.df <- as.data.frame(as.matrix(news.tdm1))
# sum rows
corp.df <- data.frame(word=rownames(corp.df), sum=rowSums(corp.df))
# sort by frequency
corp.df <- corp.df[order(-corp.df$sum),]
# fix factors so it plots in decreasing order
corp.df$word <- factor(corp.df$word, 
                     levels=with(corp.df, word[order(sum, word, decreasing = TRUE)]))
# prepare histogram of most common words
g1 <- ggplot(corp.df[1:15,], aes(x=word, y=sum)) +
      geom_bar(stat="identity")
g1
```


#### WordCloud of Common Words

```{r, echo=FALSE}
library(wordcloud)
set.seed(4)
wordcloud(corp.df$word, corp.df$sum, max.words=200, colors=brewer.pal(10, "Paired") )
```


#### Cummulative Frequency Plot -  of the corpus to give 50% and 90% of words in text.

```{r}
# add index and cummulative sum to tdm
corp.df$x <- seq(along.with=corp.df$sum)
corp.df$cs <- cumsum(corp.df$sum)
temp <- sum(corp.df$sum)
corp.df$cf <- corp.df$cs/temp
cf.v50 <- length(corp.df$cf[corp.df$cf<=0.5])/length(corp.df$cf)
cf.v90 <- length(corp.df$cf[corp.df$cf<=0.9])/length(corp.df$cf)
rm(temp)

g2 <- ggplot(corp.df, aes(x=(x/length(corp.df$sum)), y=cf)) +
      geom_line() +
      geom_vline(xintercept=cf.v50, color="blue", linetype="dashed") +
      geom_vline(xintercept=cf.v90, color="red", linetype="dashed") +
      labs(title="Word Usage Frequency", x="Terms in Corpus, %", y="Cummulative Terms in Text, %")
g2

```

#### Histogram of Word Length

```{r}
corp.df$nlet <- nchar(as.character(corp.df$word))
g4 <- ggplot(corp.df, aes(x=nlet)) +
      geom_histogram(binwidth = 1) +
      geom_vline(xintercept=mean(corp.df$nlet), color="red", linetype="dashed") +
      labs(x="Number of Letters in Word", y="Number of Words")
g4
```



### Findings

From the exploratory review the following comments are noted for consideration in the predictive algorithm.

* In the Word Usage Frequency plot we find that 50% of the words used come less than 5% of the corpus, and 90% of the words used come from about 40% of the corpus.   This indicates that removal of the least common words will potentially have a minimal effect on prediction accuracy.

* The text processing times can be very long; removal of the infrequently used words improves processing speed.


### Next Steps

1. Develop the testing and evaluation methods.
2. Develop routines for preparation of the text prediction tables.
3. Develop the data predicition algorithm.
4. Test and iterate on algorithms to improve prediction capabilities.
5. Develop shiny app.
6. Final Report
7. Bask in the glory of having finished the program, then find a job.

